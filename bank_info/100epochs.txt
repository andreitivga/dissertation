2024-01-28 13:23:37.136271: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-01-28 13:23:37.136338: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-01-28 13:23:37.138178: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-28 13:23:38.669634: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
CUDA backend failed to initialize: Found cuBLAS version 120103, but JAX was built against version 120205, which is newer. The copy of cuBLAS that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
01/28/2024 13:23:43 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
01/28/2024 13:23:43 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=18,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_model_gpt2_100epoch/runs/Jan28_13-23-43_99e8d5cfffd3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=25.0,
optim=adamw_torch,
optim_args=None,
output_dir=output_model_gpt2_100epoch,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=3,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=output_model_gpt2_100epoch,
save_on_each_node=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
/usr/local/lib/python3.10/dist-packages/datasets/load.py:2483: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
Using custom data configuration default-785d1a2b1b6db9c0
01/28/2024 13:23:43 - INFO - datasets.builder - Using custom data configuration default-785d1a2b1b6db9c0
Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text
01/28/2024 13:23:43 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text
Generating dataset text (/root/.cache/huggingface/datasets/text/default-785d1a2b1b6db9c0/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)
01/28/2024 13:23:43 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-785d1a2b1b6db9c0/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)
Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-785d1a2b1b6db9c0/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34...
01/28/2024 13:23:43 - INFO - datasets.builder - Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-785d1a2b1b6db9c0/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34...
Downloading took 0.0 min
01/28/2024 13:23:43 - INFO - datasets.download.download_manager - Downloading took 0.0 min
Checksum Computation took 0.0 min
01/28/2024 13:23:43 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
Generating train split
01/28/2024 13:23:43 - INFO - datasets.builder - Generating train split
Generating train split: 6827 examples [00:00, 78733.73 examples/s]
Generating validation split
01/28/2024 13:23:43 - INFO - datasets.builder - Generating validation split
Generating validation split: 1650 examples [00:00, 162726.65 examples/s]
Unable to verify splits sizes.
01/28/2024 13:23:43 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-785d1a2b1b6db9c0/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34. Subsequent calls will reuse this data.
01/28/2024 13:23:43 - INFO - datasets.builder - Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-785d1a2b1b6db9c0/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34. Subsequent calls will reuse this data.
[INFO|configuration_utils.py:715] 2024-01-28 13:23:43,620 >> loading configuration file output_model_gpt2_75epoch/config.json
[INFO|configuration_utils.py:777] 2024-01-28 13:23:43,621 >> Model config GPT2Config {
  "_name_or_path": "output_model_gpt2_75epoch",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.35.2",
  "use_cache": true,
  "vocab_size": 50267
}

[INFO|tokenization_utils_base.py:2020] 2024-01-28 13:23:43,641 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2020] 2024-01-28 13:23:43,641 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2020] 2024-01-28 13:23:43,642 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2020] 2024-01-28 13:23:43,642 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2020] 2024-01-28 13:23:43,642 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2020] 2024-01-28 13:23:43,642 >> loading file tokenizer_config.json
[WARNING|logging.py:314] 2024-01-28 13:23:43,712 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3118] 2024-01-28 13:23:43,718 >> loading weights file output_model_gpt2_75epoch/model.safetensors
[INFO|configuration_utils.py:791] 2024-01-28 13:23:43,748 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:3950] 2024-01-28 13:23:45,465 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:3958] 2024-01-28 13:23:45,465 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at output_model_gpt2_75epoch.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:749] 2024-01-28 13:23:45,467 >> loading configuration file output_model_gpt2_75epoch/generation_config.json
[INFO|configuration_utils.py:791] 2024-01-28 13:23:45,467 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Running tokenizer on dataset:   0% 0/6827 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-785d1a2b1b6db9c0/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-1e9632c7fe49035b.arrow
01/28/2024 13:23:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-785d1a2b1b6db9c0/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-1e9632c7fe49035b.arrow
Running tokenizer on dataset: 100% 6827/6827 [00:02<00:00, 2469.77 examples/s]
Running tokenizer on dataset:   0% 0/1650 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-785d1a2b1b6db9c0/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-9b4fa2b5198bdcdd.arrow
01/28/2024 13:23:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-785d1a2b1b6db9c0/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-9b4fa2b5198bdcdd.arrow
Running tokenizer on dataset: 100% 1650/1650 [00:00<00:00, 2031.27 examples/s]
Grouping texts in chunks of 1024:   0% 0/6827 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-785d1a2b1b6db9c0/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-52c462f6eb899376.arrow
01/28/2024 13:23:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-785d1a2b1b6db9c0/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-52c462f6eb899376.arrow
Grouping texts in chunks of 1024: 100% 6827/6827 [00:01<00:00, 3736.22 examples/s]
Grouping texts in chunks of 1024:   0% 0/1650 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-785d1a2b1b6db9c0/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-a9b10f3c98a3af2e.arrow
01/28/2024 13:23:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-785d1a2b1b6db9c0/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-a9b10f3c98a3af2e.arrow
Grouping texts in chunks of 1024: 100% 1650/1650 [00:00<00:00, 2087.55 examples/s]
Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 17.0MB/s]
[INFO|trainer.py:593] 2024-01-28 13:23:52,948 >> Using auto half precision backend
[INFO|trainer.py:1723] 2024-01-28 13:23:53,213 >> ***** Running training *****
[INFO|trainer.py:1724] 2024-01-28 13:23:53,213 >>   Num examples = 1,246
[INFO|trainer.py:1725] 2024-01-28 13:23:53,213 >>   Num Epochs = 25
[INFO|trainer.py:1726] 2024-01-28 13:23:53,213 >>   Instantaneous batch size per device = 3
[INFO|trainer.py:1729] 2024-01-28 13:23:53,214 >>   Total train batch size (w. parallel, distributed & accumulation) = 54
[INFO|trainer.py:1730] 2024-01-28 13:23:53,214 >>   Gradient Accumulation steps = 18
[INFO|trainer.py:1731] 2024-01-28 13:23:53,214 >>   Total optimization steps = 575
[INFO|trainer.py:1732] 2024-01-28 13:23:53,214 >>   Number of trainable parameters = 124,447,488
{'loss': 0.0176, 'learning_rate': 0.00013043478260869564, 'epoch': 21.63}
 87% 500/575 [1:16:36<11:31,  9.21s/it][INFO|trainer.py:2881] 2024-01-28 14:40:29,428 >> Saving model checkpoint to output_model_gpt2_100epoch/checkpoint-500
[INFO|configuration_utils.py:461] 2024-01-28 14:40:29,429 >> Configuration saved in output_model_gpt2_100epoch/checkpoint-500/config.json
[INFO|configuration_utils.py:564] 2024-01-28 14:40:29,430 >> Configuration saved in output_model_gpt2_100epoch/checkpoint-500/generation_config.json
[INFO|modeling_utils.py:2193] 2024-01-28 14:40:31,088 >> Model weights saved in output_model_gpt2_100epoch/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-28 14:40:31,089 >> tokenizer config file saved in output_model_gpt2_100epoch/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-28 14:40:31,089 >> Special tokens file saved in output_model_gpt2_100epoch/checkpoint-500/special_tokens_map.json
100% 575/575 [1:28:11<00:00,  9.21s/it][INFO|trainer.py:1955] 2024-01-28 14:52:04,557 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 5291.3732, 'train_samples_per_second': 5.887, 'train_steps_per_second': 0.109, 'train_loss': 0.01633945962657099, 'epoch': 24.88}
100% 575/575 [1:28:11<00:00,  9.20s/it]
[INFO|trainer.py:2881] 2024-01-28 14:52:04,591 >> Saving model checkpoint to output_model_gpt2_100epoch
[INFO|configuration_utils.py:461] 2024-01-28 14:52:04,593 >> Configuration saved in output_model_gpt2_100epoch/config.json
[INFO|configuration_utils.py:564] 2024-01-28 14:52:04,594 >> Configuration saved in output_model_gpt2_100epoch/generation_config.json
[INFO|modeling_utils.py:2193] 2024-01-28 14:52:06,808 >> Model weights saved in output_model_gpt2_100epoch/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-28 14:52:06,809 >> tokenizer config file saved in output_model_gpt2_100epoch/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-28 14:52:06,810 >> Special tokens file saved in output_model_gpt2_100epoch/special_tokens_map.json
***** train metrics *****
  epoch                    =      24.88
  train_loss               =     0.0163
  train_runtime            = 1:28:11.37
  train_samples            =       1246
  train_samples_per_second =      5.887
  train_steps_per_second   =      0.109
01/28/2024 14:52:06 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3158] 2024-01-28 14:52:06,888 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-28 14:52:06,888 >>   Num examples = 379
[INFO|trainer.py:3163] 2024-01-28 14:52:06,888 >>   Batch size = 8
100% 48/48 [00:27<00:00,  1.73it/s]
***** eval metrics *****
  epoch                   =      24.88
  eval_accuracy           =     0.5211
  eval_loss               =     6.5943
  eval_runtime            = 0:00:28.50
  eval_samples            =        379
  eval_samples_per_second =     13.294
  eval_steps_per_second   =      1.684
  perplexity              =    730.935
[INFO|modelcard.py:452] 2024-01-28 14:52:35,578 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.5211429986304443}]}