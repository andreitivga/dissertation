2024-01-27 18:08:03.171310: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-01-27 18:08:03.171367: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-01-27 18:08:03.172630: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-27 18:08:04.613571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
CUDA backend failed to initialize: Found cuBLAS version 120103, but JAX was built against version 120205, which is newer. The copy of cuBLAS that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
01/27/2024 18:08:08 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
01/27/2024 18:08:08 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=18,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_model_gpt2_10epoch/runs/Jan27_18-08-08_f9b22a5f3ad7,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=10.0,
optim=adamw_torch,
optim_args=None,
output_dir=output_model_gpt2_10epoch,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=3,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=output_model_gpt2_10epoch,
save_on_each_node=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
/usr/local/lib/python3.10/dist-packages/datasets/load.py:2483: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
Using custom data configuration default-abb477c100b88dba
01/27/2024 18:08:08 - INFO - datasets.builder - Using custom data configuration default-abb477c100b88dba
Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text
01/27/2024 18:08:08 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text
Generating dataset text (/root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)
01/27/2024 18:08:08 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)
Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34...
01/27/2024 18:08:08 - INFO - datasets.builder - Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34...
Downloading took 0.0 min
01/27/2024 18:08:08 - INFO - datasets.download.download_manager - Downloading took 0.0 min
Checksum Computation took 0.0 min
01/27/2024 18:08:08 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
Generating train split
01/27/2024 18:08:08 - INFO - datasets.builder - Generating train split
Generating train split: 6827 examples [00:00, 75940.12 examples/s]
Generating validation split
01/27/2024 18:08:08 - INFO - datasets.builder - Generating validation split
Generating validation split: 1650 examples [00:00, 148817.34 examples/s]
Unable to verify splits sizes.
01/27/2024 18:08:08 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34. Subsequent calls will reuse this data.
01/27/2024 18:08:08 - INFO - datasets.builder - Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34. Subsequent calls will reuse this data.
config.json: 100% 665/665 [00:00<00:00, 3.40MB/s]
[INFO|configuration_utils.py:717] 2024-01-27 18:08:08,757 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json
[INFO|configuration_utils.py:777] 2024-01-27 18:08:08,758 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.35.2",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_auto.py:566] 2024-01-27 18:08:08,805 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:717] 2024-01-27 18:08:08,851 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json
[INFO|configuration_utils.py:777] 2024-01-27 18:08:08,852 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.35.2",
  "use_cache": true,
  "vocab_size": 50257
}

vocab.json: 100% 1.04M/1.04M [00:00<00:00, 9.67MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 8.41MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 29.4MB/s]
[INFO|tokenization_utils_base.py:2022] 2024-01-27 18:08:09,598 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/vocab.json
[INFO|tokenization_utils_base.py:2022] 2024-01-27 18:08:09,598 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/merges.txt
[INFO|tokenization_utils_base.py:2022] 2024-01-27 18:08:09,598 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/tokenizer.json
[INFO|tokenization_utils_base.py:2022] 2024-01-27 18:08:09,598 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2022] 2024-01-27 18:08:09,598 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2022] 2024-01-27 18:08:09,598 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:717] 2024-01-27 18:08:09,598 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json
[INFO|configuration_utils.py:777] 2024-01-27 18:08:09,599 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.35.2",
  "use_cache": true,
  "vocab_size": 50257
}

model.safetensors: 100% 548M/548M [00:03<00:00, 180MB/s]
[INFO|modeling_utils.py:3121] 2024-01-27 18:08:12,944 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/model.safetensors
[INFO|configuration_utils.py:791] 2024-01-27 18:08:13,095 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:3950] 2024-01-27 18:08:15,211 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:3958] 2024-01-27 18:08:15,211 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
generation_config.json: 100% 124/124 [00:00<00:00, 631kB/s]
[INFO|configuration_utils.py:751] 2024-01-27 18:08:15,309 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/generation_config.json
[INFO|configuration_utils.py:791] 2024-01-27 18:08:15,309 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:1648] 2024-01-27 18:08:15,334 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50267. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Running tokenizer on dataset:   0% 0/6827 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-32815de09adf3714.arrow
01/27/2024 18:08:16 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-32815de09adf3714.arrow
Running tokenizer on dataset: 100% 6827/6827 [00:02<00:00, 2454.16 examples/s]
Running tokenizer on dataset:   0% 0/1650 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-e6e55c630505af0d.arrow
01/27/2024 18:08:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-e6e55c630505af0d.arrow
Running tokenizer on dataset: 100% 1650/1650 [00:00<00:00, 1957.75 examples/s]
Grouping texts in chunks of 1024:   0% 0/6827 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-02e484b4e058c50a.arrow
01/27/2024 18:08:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-02e484b4e058c50a.arrow
Grouping texts in chunks of 1024: 100% 6827/6827 [00:01<00:00, 4472.11 examples/s]
Grouping texts in chunks of 1024:   0% 0/1650 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-34e48c231779940f.arrow
01/27/2024 18:08:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-34e48c231779940f.arrow
Grouping texts in chunks of 1024: 100% 1650/1650 [00:00<00:00, 3244.68 examples/s]
Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 14.1MB/s]
[INFO|trainer.py:593] 2024-01-27 18:08:22,656 >> Using auto half precision backend
[INFO|trainer.py:1723] 2024-01-27 18:08:22,923 >> ***** Running training *****
[INFO|trainer.py:1724] 2024-01-27 18:08:22,923 >>   Num examples = 1,246
[INFO|trainer.py:1725] 2024-01-27 18:08:22,923 >>   Num Epochs = 10
[INFO|trainer.py:1726] 2024-01-27 18:08:22,923 >>   Instantaneous batch size per device = 3
[INFO|trainer.py:1729] 2024-01-27 18:08:22,923 >>   Total train batch size (w. parallel, distributed & accumulation) = 54
[INFO|trainer.py:1730] 2024-01-27 18:08:22,924 >>   Gradient Accumulation steps = 18
[INFO|trainer.py:1731] 2024-01-27 18:08:22,924 >>   Total optimization steps = 230
[INFO|trainer.py:1732] 2024-01-27 18:08:22,924 >>   Number of trainable parameters = 124,447,488
100% 230/230 [34:16<00:00,  8.96s/it][INFO|trainer.py:1955] 2024-01-27 18:42:39,388 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 2056.4974, 'train_samples_per_second': 6.059, 'train_steps_per_second': 0.112, 'train_loss': 4.234929623811142, 'epoch': 9.95}
100% 230/230 [34:16<00:00,  8.94s/it]
[INFO|trainer.py:2881] 2024-01-27 18:42:39,424 >> Saving model checkpoint to output_model_gpt2_10epoch
[INFO|configuration_utils.py:461] 2024-01-27 18:42:39,425 >> Configuration saved in output_model_gpt2_10epoch/config.json
[INFO|configuration_utils.py:564] 2024-01-27 18:42:39,425 >> Configuration saved in output_model_gpt2_10epoch/generation_config.json
[INFO|modeling_utils.py:2193] 2024-01-27 18:42:43,794 >> Model weights saved in output_model_gpt2_10epoch/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-27 18:42:43,795 >> tokenizer config file saved in output_model_gpt2_10epoch/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-27 18:42:43,795 >> Special tokens file saved in output_model_gpt2_10epoch/special_tokens_map.json
***** train metrics *****
  epoch                    =       9.95
  train_loss               =     4.2349
  train_runtime            = 0:34:16.49
  train_samples            =       1246
  train_samples_per_second =      6.059
  train_steps_per_second   =      0.112
01/27/2024 18:42:43 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3158] 2024-01-27 18:42:43,853 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-27 18:42:43,853 >>   Num examples = 379
[INFO|trainer.py:3163] 2024-01-27 18:42:43,853 >>   Batch size = 8
100% 48/48 [00:25<00:00,  1.88it/s]
***** eval metrics *****
  epoch                   =       9.95
  eval_accuracy           =     0.5418
  eval_loss               =     3.6626
  eval_runtime            = 0:00:26.24
  eval_samples            =        379
  eval_samples_per_second =      14.44
  eval_steps_per_second   =      1.829
  perplexity              =    38.9644
[INFO|modelcard.py:452] 2024-01-27 18:43:10,574 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.541766288297908}]}