2024-01-27 18:48:25.899361: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-01-27 18:48:25.899415: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-01-27 18:48:25.900872: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-27 18:48:27.018026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
CUDA backend failed to initialize: Found cuBLAS version 120103, but JAX was built against version 120205, which is newer. The copy of cuBLAS that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
01/27/2024 18:48:29 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
01/27/2024 18:48:29 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=18,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_model_gpt2_20epoch/runs/Jan27_18-48-29_f9b22a5f3ad7,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=10.0,
optim=adamw_torch,
optim_args=None,
output_dir=output_model_gpt2_20epoch,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=3,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=output_model_gpt2_20epoch,
save_on_each_node=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
/usr/local/lib/python3.10/dist-packages/datasets/load.py:2483: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
Using custom data configuration default-abb477c100b88dba
01/27/2024 18:48:29 - INFO - datasets.builder - Using custom data configuration default-abb477c100b88dba
Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text
01/27/2024 18:48:29 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text
Overwrite dataset info from restored data version if exists.
01/27/2024 18:48:29 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34
01/27/2024 18:48:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34
Found cached dataset text (/root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)
01/27/2024 18:48:29 - INFO - datasets.builder - Found cached dataset text (/root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)
Loading Dataset info from /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34
01/27/2024 18:48:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34
[INFO|configuration_utils.py:715] 2024-01-27 18:48:29,719 >> loading configuration file output_model_gpt2_10epoch/config.json
[INFO|configuration_utils.py:777] 2024-01-27 18:48:29,720 >> Model config GPT2Config {
  "_name_or_path": "output_model_gpt2_10epoch",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.35.2",
  "use_cache": true,
  "vocab_size": 50267
}

[INFO|tokenization_utils_base.py:2020] 2024-01-27 18:48:29,729 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2020] 2024-01-27 18:48:29,729 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2020] 2024-01-27 18:48:29,729 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2020] 2024-01-27 18:48:29,729 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2020] 2024-01-27 18:48:29,729 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2020] 2024-01-27 18:48:29,729 >> loading file tokenizer_config.json
[WARNING|logging.py:314] 2024-01-27 18:48:29,800 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3118] 2024-01-27 18:48:29,806 >> loading weights file output_model_gpt2_10epoch/model.safetensors
[INFO|configuration_utils.py:791] 2024-01-27 18:48:29,812 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:3950] 2024-01-27 18:48:31,495 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:3958] 2024-01-27 18:48:31,496 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at output_model_gpt2_10epoch.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:749] 2024-01-27 18:48:31,497 >> loading configuration file output_model_gpt2_10epoch/generation_config.json
[INFO|configuration_utils.py:791] 2024-01-27 18:48:31,497 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Running tokenizer on dataset:   0% 0/6827 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-7bcae009cccdd03a.arrow
01/27/2024 18:48:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-7bcae009cccdd03a.arrow
Running tokenizer on dataset: 100% 6827/6827 [00:04<00:00, 1618.87 examples/s]
Running tokenizer on dataset:   0% 0/1650 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-75b3c1e87d66194b.arrow
01/27/2024 18:48:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-75b3c1e87d66194b.arrow
Running tokenizer on dataset: 100% 1650/1650 [00:01<00:00, 1177.54 examples/s]
Grouping texts in chunks of 1024:   0% 0/6827 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-b2c5d7efdf94b6fa.arrow
01/27/2024 18:48:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-b2c5d7efdf94b6fa.arrow
Grouping texts in chunks of 1024: 100% 6827/6827 [00:02<00:00, 3306.82 examples/s]
Grouping texts in chunks of 1024:   0% 0/1650 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-0567255aac3909cd.arrow
01/27/2024 18:48:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-abb477c100b88dba/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-0567255aac3909cd.arrow
Grouping texts in chunks of 1024: 100% 1650/1650 [00:00<00:00, 3660.60 examples/s]
[INFO|trainer.py:593] 2024-01-27 18:48:40,517 >> Using auto half precision backend
[INFO|trainer.py:1723] 2024-01-27 18:48:40,751 >> ***** Running training *****
[INFO|trainer.py:1724] 2024-01-27 18:48:40,751 >>   Num examples = 1,246
[INFO|trainer.py:1725] 2024-01-27 18:48:40,751 >>   Num Epochs = 10
[INFO|trainer.py:1726] 2024-01-27 18:48:40,751 >>   Instantaneous batch size per device = 3
[INFO|trainer.py:1729] 2024-01-27 18:48:40,751 >>   Total train batch size (w. parallel, distributed & accumulation) = 54
[INFO|trainer.py:1730] 2024-01-27 18:48:40,751 >>   Gradient Accumulation steps = 18
[INFO|trainer.py:1731] 2024-01-27 18:48:40,751 >>   Total optimization steps = 230
[INFO|trainer.py:1732] 2024-01-27 18:48:40,751 >>   Number of trainable parameters = 124,447,488
100% 230/230 [34:13<00:00,  8.95s/it][INFO|trainer.py:1955] 2024-01-27 19:22:53,771 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 2053.0507, 'train_samples_per_second': 6.069, 'train_steps_per_second': 0.112, 'train_loss': 0.21474625960640284, 'epoch': 9.95}
100% 230/230 [34:13<00:00,  8.93s/it]
[INFO|trainer.py:2881] 2024-01-27 19:22:53,805 >> Saving model checkpoint to output_model_gpt2_20epoch
[INFO|configuration_utils.py:461] 2024-01-27 19:22:53,807 >> Configuration saved in output_model_gpt2_20epoch/config.json
[INFO|configuration_utils.py:564] 2024-01-27 19:22:53,807 >> Configuration saved in output_model_gpt2_20epoch/generation_config.json
[INFO|modeling_utils.py:2193] 2024-01-27 19:22:55,960 >> Model weights saved in output_model_gpt2_20epoch/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-01-27 19:22:55,961 >> tokenizer config file saved in output_model_gpt2_20epoch/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-01-27 19:22:55,961 >> Special tokens file saved in output_model_gpt2_20epoch/special_tokens_map.json
***** train metrics *****
  epoch                    =       9.95
  train_loss               =     0.2147
  train_runtime            = 0:34:13.05
  train_samples            =       1246
  train_samples_per_second =      6.069
  train_steps_per_second   =      0.112
01/27/2024 19:22:56 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3158] 2024-01-27 19:22:56,013 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-01-27 19:22:56,013 >>   Num examples = 379
[INFO|trainer.py:3163] 2024-01-27 19:22:56,013 >>   Batch size = 8
100% 48/48 [00:25<00:00,  1.88it/s]
***** eval metrics *****
  epoch                   =       9.95
  eval_accuracy           =     0.5334
  eval_loss               =     4.6565
  eval_runtime            = 0:00:26.24
  eval_samples            =        379
  eval_samples_per_second =     14.441
  eval_steps_per_second   =      1.829
  perplexity              =   105.2648
[INFO|modelcard.py:452] 2024-01-27 19:23:22,407 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.5334096776772749}]}