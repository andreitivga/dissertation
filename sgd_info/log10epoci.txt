2023-06-15 16:46:11.789015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
06/15/2023 16:46:15 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
06/15/2023 16:46:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=18,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_model_gpt2_10epoch/runs/Jun15_16-46-13_bc3229eab082,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=10.0,
optim=adamw_hf,
optim_args=None,
output_dir=output_model_gpt2_10epoch,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=3,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=output_model_gpt2_10epoch,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
06/15/2023 16:46:16 - INFO - datasets.builder - Using custom data configuration default-7136d3cbf4d4fe15
06/15/2023 16:46:16 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text
06/15/2023 16:46:16 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
06/15/2023 16:46:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-7136d3cbf4d4fe15/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2
06/15/2023 16:46:16 - WARNING - datasets.builder - Found cached dataset text (/root/.cache/huggingface/datasets/text/default-7136d3cbf4d4fe15/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)
06/15/2023 16:46:16 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-7136d3cbf4d4fe15/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2
100% 2/2 [00:00<00:00, 415.22it/s]
[INFO|configuration_utils.py:669] 2023-06-15 16:46:17,176 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json
[INFO|configuration_utils.py:725] 2023-06-15 16:46:17,177 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_auto.py:503] 2023-06-15 16:46:17,411 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:669] 2023-06-15 16:46:17,631 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json
[INFO|configuration_utils.py:725] 2023-06-15 16:46:17,631 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1823] 2023-06-15 16:46:18,103 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/vocab.json
[INFO|tokenization_utils_base.py:1823] 2023-06-15 16:46:18,103 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/merges.txt
[INFO|tokenization_utils_base.py:1823] 2023-06-15 16:46:18,103 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/tokenizer.json
[INFO|tokenization_utils_base.py:1823] 2023-06-15 16:46:18,103 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1823] 2023-06-15 16:46:18,103 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1823] 2023-06-15 16:46:18,103 >> loading file tokenizer_config.json from cache at None
[INFO|configuration_utils.py:669] 2023-06-15 16:46:18,103 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json
[INFO|configuration_utils.py:725] 2023-06-15 16:46:18,104 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:2578] 2023-06-15 16:46:18,173 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/model.safetensors
[INFO|configuration_utils.py:577] 2023-06-15 16:46:18,182 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 50256,
  "eos_token_id": 50256,
  "transformers_version": "4.30.2"
}

[INFO|modeling_utils.py:3295] 2023-06-15 16:46:19,769 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:3303] 2023-06-15 16:46:19,769 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:539] 2023-06-15 16:46:19,995 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/generation_config.json
[INFO|configuration_utils.py:577] 2023-06-15 16:46:19,996 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 50256,
  "eos_token_id": 50256,
  "transformers_version": "4.30.2"
}

06/15/2023 16:46:20 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-7136d3cbf4d4fe15/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-b644e0b31aa02e53.arrow
Running tokenizer on dataset:   0% 0/24363 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:3594] 2023-06-15 16:46:20,190 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1118 > 1024). Running this sequence through the model will result in indexing errors
[WARNING|run_clm.py:445] 2023-06-15 16:46:20,190 >> ^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.
06/15/2023 16:46:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-7136d3cbf4d4fe15/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-0e5ad1ba0bc0d042.arrow
06/15/2023 16:46:24 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-7136d3cbf4d4fe15/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-43e1ba127e819ab1.arrow
Grouping texts in chunks of 1024:   0% 0/24363 [00:00<?, ? examples/s]06/15/2023 16:46:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-7136d3cbf4d4fe15/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-f8bb9bb4024802e9.arrow
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1786] 2023-06-15 16:46:34,121 >> ***** Running training *****
[INFO|trainer.py:1787] 2023-06-15 16:46:34,121 >>   Num examples = 49,443
[INFO|trainer.py:1788] 2023-06-15 16:46:34,121 >>   Num Epochs = 10
[INFO|trainer.py:1789] 2023-06-15 16:46:34,122 >>   Instantaneous batch size per device = 3
[INFO|trainer.py:1790] 2023-06-15 16:46:34,122 >>   Total train batch size (w. parallel, distributed & accumulation) = 54
[INFO|trainer.py:1791] 2023-06-15 16:46:34,122 >>   Gradient Accumulation steps = 18
[INFO|trainer.py:1792] 2023-06-15 16:46:34,122 >>   Total optimization steps = 9,150
[INFO|trainer.py:1793] 2023-06-15 16:46:34,122 >>   Number of trainable parameters = 124,439,808
{'loss': 0.9896, 'learning_rate': 0.0009456830601092896, 'epoch': 0.55}
  5% 500/9150 [15:10<4:23:14,  1.83s/it][INFO|trainer.py:2926] 2023-06-15 17:01:44,996 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-500
[INFO|configuration_utils.py:458] 2023-06-15 17:01:44,996 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-500/config.json
[INFO|configuration_utils.py:364] 2023-06-15 17:01:44,997 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-500/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 17:01:45,668 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 17:01:45,668 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 17:01:45,668 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-500/special_tokens_map.json
{'loss': 0.5947, 'learning_rate': 0.0008910382513661202, 'epoch': 1.09}
 11% 1000/9150 [30:23<4:07:01,  1.82s/it][INFO|trainer.py:2926] 2023-06-15 17:16:57,752 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-1000
[INFO|configuration_utils.py:458] 2023-06-15 17:16:57,753 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-1000/config.json
[INFO|configuration_utils.py:364] 2023-06-15 17:16:57,754 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-1000/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 17:16:58,393 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-1000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 17:16:58,394 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 17:16:58,394 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-1000/special_tokens_map.json
{'loss': 0.4829, 'learning_rate': 0.0008363934426229508, 'epoch': 1.64}
 16% 1500/9150 [45:35<3:51:51,  1.82s/it][INFO|trainer.py:2926] 2023-06-15 17:32:09,591 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-1500
[INFO|configuration_utils.py:458] 2023-06-15 17:32:09,592 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-1500/config.json
[INFO|configuration_utils.py:364] 2023-06-15 17:32:09,593 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-1500/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 17:32:10,229 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-1500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 17:32:10,230 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 17:32:10,230 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-1500/special_tokens_map.json
{'loss': 0.402, 'learning_rate': 0.0007817486338797814, 'epoch': 2.18}
 22% 2000/9150 [1:00:47<3:36:34,  1.82s/it][INFO|trainer.py:2926] 2023-06-15 17:47:21,598 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-2000
[INFO|configuration_utils.py:458] 2023-06-15 17:47:21,599 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-2000/config.json
[INFO|configuration_utils.py:364] 2023-06-15 17:47:21,600 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-2000/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 17:47:22,239 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-2000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 17:47:22,240 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 17:47:22,240 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-2000/special_tokens_map.json
{'loss': 0.3359, 'learning_rate': 0.000727103825136612, 'epoch': 2.73}
 27% 2500/9150 [1:15:59<3:21:47,  1.82s/it][INFO|trainer.py:2926] 2023-06-15 18:02:33,351 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-2500
[INFO|configuration_utils.py:458] 2023-06-15 18:02:33,351 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-2500/config.json
[INFO|configuration_utils.py:364] 2023-06-15 18:02:33,352 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-2500/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 18:02:33,990 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-2500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 18:02:33,990 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-2500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 18:02:33,991 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-2500/special_tokens_map.json
{'loss': 0.2849, 'learning_rate': 0.0006724590163934426, 'epoch': 3.28}
 33% 3000/9150 [1:31:12<3:06:33,  1.82s/it][INFO|trainer.py:2926] 2023-06-15 18:17:46,697 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-3000
[INFO|configuration_utils.py:458] 2023-06-15 18:17:46,697 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-3000/config.json
[INFO|configuration_utils.py:364] 2023-06-15 18:17:46,698 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-3000/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 18:17:47,338 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-3000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 18:17:47,338 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-3000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 18:17:47,339 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-3000/special_tokens_map.json
{'loss': 0.2497, 'learning_rate': 0.0006178142076502732, 'epoch': 3.82}
 38% 3500/9150 [1:46:24<2:51:25,  1.82s/it][INFO|trainer.py:2926] 2023-06-15 18:32:58,792 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-3500
[INFO|configuration_utils.py:458] 2023-06-15 18:32:58,793 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-3500/config.json
[INFO|configuration_utils.py:364] 2023-06-15 18:32:58,793 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-3500/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 18:32:59,434 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-3500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 18:32:59,434 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-3500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 18:32:59,434 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-3500/special_tokens_map.json
{'loss': 0.2169, 'learning_rate': 0.0005631693989071038, 'epoch': 4.37}
 44% 4000/9150 [2:01:37<2:36:17,  1.82s/it][INFO|trainer.py:2926] 2023-06-15 18:48:11,200 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-4000
[INFO|configuration_utils.py:458] 2023-06-15 18:48:11,201 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-4000/config.json
[INFO|configuration_utils.py:364] 2023-06-15 18:48:11,202 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-4000/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 18:48:11,844 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-4000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 18:48:11,844 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-4000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 18:48:11,844 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-4000/special_tokens_map.json
{'loss': 0.1988, 'learning_rate': 0.0005085245901639345, 'epoch': 4.91}
 49% 4500/9150 [2:16:49<2:21:38,  1.83s/it][INFO|trainer.py:2926] 2023-06-15 19:03:23,434 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-4500
[INFO|configuration_utils.py:458] 2023-06-15 19:03:23,435 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-4500/config.json
[INFO|configuration_utils.py:364] 2023-06-15 19:03:23,436 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-4500/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 19:03:24,078 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-4500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 19:03:24,079 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-4500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 19:03:24,079 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-4500/special_tokens_map.json
{'loss': 0.1748, 'learning_rate': 0.00045387978142076503, 'epoch': 5.46}
 55% 5000/9150 [2:32:03<2:05:48,  1.82s/it][INFO|trainer.py:2926] 2023-06-15 19:18:37,659 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-5000
[INFO|configuration_utils.py:458] 2023-06-15 19:18:37,660 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-5000/config.json
[INFO|configuration_utils.py:364] 2023-06-15 19:18:37,660 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-5000/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 19:18:38,297 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-5000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 19:18:38,297 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-5000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 19:18:38,298 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-5000/special_tokens_map.json
{'loss': 0.1652, 'learning_rate': 0.00039923497267759563, 'epoch': 6.01}
 60% 5500/9150 [2:47:15<1:50:53,  1.82s/it][INFO|trainer.py:2926] 2023-06-15 19:33:49,893 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-5500
[INFO|configuration_utils.py:458] 2023-06-15 19:33:49,894 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-5500/config.json
[INFO|configuration_utils.py:364] 2023-06-15 19:33:49,894 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-5500/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 19:33:50,533 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-5500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 19:33:50,533 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-5500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 19:33:50,533 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-5500/special_tokens_map.json
{'loss': 0.1469, 'learning_rate': 0.00034459016393442623, 'epoch': 6.55}
 66% 6000/9150 [3:02:33<1:35:36,  1.82s/it][INFO|trainer.py:2926] 2023-06-15 19:49:07,129 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-6000
[INFO|configuration_utils.py:458] 2023-06-15 19:49:07,130 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-6000/config.json
[INFO|configuration_utils.py:364] 2023-06-15 19:49:07,130 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-6000/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 19:49:07,776 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-6000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 19:49:07,777 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-6000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 19:49:07,777 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-6000/special_tokens_map.json
{'loss': 0.14, 'learning_rate': 0.00028994535519125684, 'epoch': 7.1}
 71% 6500/9150 [3:17:44<1:20:21,  1.82s/it][INFO|trainer.py:2926] 2023-06-15 20:04:18,428 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-6500
[INFO|configuration_utils.py:458] 2023-06-15 20:04:18,429 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-6500/config.json
[INFO|configuration_utils.py:364] 2023-06-15 20:04:18,429 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-6500/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 20:04:19,073 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-6500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 20:04:19,073 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-6500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 20:04:19,074 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-6500/special_tokens_map.json
{'loss': 0.1281, 'learning_rate': 0.00023530054644808744, 'epoch': 7.65}
 77% 7000/9150 [3:32:56<1:05:22,  1.82s/it][INFO|trainer.py:2926] 2023-06-15 20:19:30,338 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-7000
[INFO|configuration_utils.py:458] 2023-06-15 20:19:30,339 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-7000/config.json
[INFO|configuration_utils.py:364] 2023-06-15 20:19:30,339 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-7000/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 20:19:31,000 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-7000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 20:19:31,001 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-7000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 20:19:31,001 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-7000/special_tokens_map.json
{'loss': 0.1221, 'learning_rate': 0.00018065573770491804, 'epoch': 8.19}
 82% 7500/9150 [3:48:13<50:08,  1.82s/it][INFO|trainer.py:2926] 2023-06-15 20:34:47,562 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-7500
[INFO|configuration_utils.py:458] 2023-06-15 20:34:47,563 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-7500/config.json
[INFO|configuration_utils.py:364] 2023-06-15 20:34:47,563 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-7500/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 20:34:48,219 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-7500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 20:34:48,219 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-7500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 20:34:48,220 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-7500/special_tokens_map.json
{'loss': 0.1145, 'learning_rate': 0.00012601092896174864, 'epoch': 8.74}
 87% 8000/9150 [4:03:26<34:54,  1.82s/it][INFO|trainer.py:2926] 2023-06-15 20:50:00,975 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-8000
[INFO|configuration_utils.py:458] 2023-06-15 20:50:00,976 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-8000/config.json
[INFO|configuration_utils.py:364] 2023-06-15 20:50:00,976 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-8000/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 20:50:01,620 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-8000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 20:50:01,621 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-8000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 20:50:01,621 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-8000/special_tokens_map.json
{'loss': 0.1086, 'learning_rate': 7.136612021857923e-05, 'epoch': 9.28}
 93% 8500/9150 [4:18:40<19:42,  1.82s/it][INFO|trainer.py:2926] 2023-06-15 21:05:14,651 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-8500
[INFO|configuration_utils.py:458] 2023-06-15 21:05:14,652 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-8500/config.json
[INFO|configuration_utils.py:364] 2023-06-15 21:05:14,653 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-8500/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 21:05:15,289 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-8500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 21:05:15,290 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-8500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 21:05:15,290 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-8500/special_tokens_map.json
{'loss': 0.1043, 'learning_rate': 1.6721311475409837e-05, 'epoch': 9.83}
 98% 9000/9150 [4:33:52<04:32,  1.82s/it][INFO|trainer.py:2926] 2023-06-15 21:20:26,196 >> Saving model checkpoint to output_model_gpt2_10epoch/checkpoint-9000
[INFO|configuration_utils.py:458] 2023-06-15 21:20:26,196 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-9000/config.json
[INFO|configuration_utils.py:364] 2023-06-15 21:20:26,197 >> Configuration saved in output_model_gpt2_10epoch/checkpoint-9000/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 21:20:26,838 >> Model weights saved in output_model_gpt2_10epoch/checkpoint-9000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 21:20:26,839 >> tokenizer config file saved in output_model_gpt2_10epoch/checkpoint-9000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 21:20:26,839 >> Special tokens file saved in output_model_gpt2_10epoch/checkpoint-9000/special_tokens_map.json
100% 9150/9150 [4:38:26<00:00,  1.82s/it][INFO|trainer.py:2053] 2023-06-15 21:25:01,087 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 16706.9648, 'train_samples_per_second': 29.594, 'train_steps_per_second': 0.548, 'train_loss': 0.27272472569199857, 'epoch': 9.99}
100% 9150/9150 [4:38:26<00:00,  1.83s/it]
[INFO|trainer.py:2926] 2023-06-15 21:25:01,089 >> Saving model checkpoint to output_model_gpt2_10epoch
[INFO|configuration_utils.py:458] 2023-06-15 21:25:01,090 >> Configuration saved in output_model_gpt2_10epoch/config.json
[INFO|configuration_utils.py:364] 2023-06-15 21:25:01,090 >> Configuration saved in output_model_gpt2_10epoch/generation_config.json
[INFO|modeling_utils.py:1853] 2023-06-15 21:25:01,728 >> Model weights saved in output_model_gpt2_10epoch/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2023-06-15 21:25:01,728 >> tokenizer config file saved in output_model_gpt2_10epoch/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2023-06-15 21:25:01,728 >> Special tokens file saved in output_model_gpt2_10epoch/special_tokens_map.json
***** train metrics *****
  epoch                    =       9.99
  train_loss               =     0.2727
  train_runtime            = 4:38:26.96
  train_samples            =      49443
  train_samples_per_second =     29.594
  train_steps_per_second   =      0.548
06/15/2023 21:25:01 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3200] 2023-06-15 21:25:01,827 >> ***** Running Evaluation *****
[INFO|trainer.py:3202] 2023-06-15 21:25:01,827 >>   Num examples = 7116
[INFO|trainer.py:3205] 2023-06-15 21:25:01,827 >>   Batch size = 8
100% 890/890 [02:00<00:00,  7.38it/s]
***** eval metrics *****
  epoch                   =       9.99
  eval_accuracy           =       0.78
  eval_loss               =     1.8948
  eval_runtime            = 0:02:00.77
  eval_samples            =       7116
  eval_samples_per_second =     58.918
  eval_steps_per_second   =      7.369
  perplexity              =     6.6515
[INFO|modelcard.py:451] 2023-06-15 21:27:03,219 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7800438151849782}]}